{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30213,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2><center> <span style = \"font-family: Babas; font-size: 2em;\"> Anomaly Detection in Credit Card Transactions </span> </center></h2>","metadata":{"id":"7ei-nx88WX3Z"}},{"cell_type":"markdown","source":"### Importing libraries","metadata":{}},{"cell_type":"code","source":"# File system manangement\nimport time, psutil, os, gc\n\n# Mathematical functions\nimport math\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Plotting and visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nsns.set_theme()\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# Train-test split\nfrom sklearn.model_selection import train_test_split\n\n# Progress bar for loop\nfrom tqdm.contrib import itertools","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:27.537188Z","iopub.execute_input":"2024-03-05T21:53:27.538213Z","iopub.status.idle":"2024-03-05T21:53:30.599799Z","shell.execute_reply.started":"2024-03-05T21:53:27.538095Z","shell.execute_reply":"2024-03-05T21:53:30.598356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Runtime and memory usage","metadata":{}},{"cell_type":"code","source":"# Recording the starting time, complemented with a stopping time check in the end to compute process runtime\nstart = time.time()\n\n# Class representing the OS process and having memory_info() method to compute process memory usage\nprocess = psutil.Process(os.getpid())","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:30.602551Z","iopub.execute_input":"2024-03-05T21:53:30.602951Z","iopub.status.idle":"2024-03-05T21:53:30.611662Z","shell.execute_reply.started":"2024-03-05T21:53:30.602903Z","shell.execute_reply":"2024-03-05T21:53:30.609268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data\n\nSource: **https://www.kaggle.com/mlg-ulb/creditcardfraud**\n\nThe dataset contains information on the transactions made using credit cards by European cardholders, in two particular days of September $2013$. It presents a total of $284807$ transactions, of which $492$ were fraudulent. Clearly, the dataset is highly imbalanced, the positive class (fraudulent transactions) accounting for only $0.173\\%$ of all transactions. The columns in the dataset are as follows:\n\n- **Time:** The time (in seconds) elapsed between the transaction and the very first transaction\n- **V1 to V28:** Obtained from principle component analysis (PCA) transformation on original features that are not available due to confidentiality\n- **Amount:** The amount of the transaction\n- **Class:** The status of the transaction with respect to authenticity. The class of an authentic (resp. fraudulent) transaction is taken to be $0$ (resp. $1$)","metadata":{"id":"zdSeQOpjWX3d"}},{"cell_type":"code","source":"# Loading the data\ndata = pd.read_csv('../input/creditcardfraud/creditcard.csv')\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(data.memory_usage().sum()/(1024*1024)),\n                 \"Dataset shape\": \"{}\".format(data.shape)}).to_string())\ndata.head()","metadata":{"id":"8Pk3D6vpWX3e","outputId":"7e521b0e-3e3e-4f95-df97-13bf5b69f10c","execution":{"iopub.status.busy":"2024-03-05T21:53:30.613626Z","iopub.execute_input":"2024-03-05T21:53:30.614143Z","iopub.status.idle":"2024-03-05T21:53:36.164486Z","shell.execute_reply.started":"2024-03-05T21:53:30.614098Z","shell.execute_reply":"2024-03-05T21:53:36.16305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train-Validation-Test Split","metadata":{"id":"crLdt9ZJWX3l"}},{"cell_type":"code","source":"# Splitting the data by target class\ndata_0, data_1 = data[data['Class'] == 0], data[data['Class'] == 1]\n\n# Feature-target split\nX_0, y_0 = data_0.drop('Class', axis = 1), data_0['Class']\nX_1, y_1 = data_1.drop('Class', axis = 1), data_1['Class']\n\n# Splitting the authentic class and constructing the training set\nX_train, X_test, y_train, y_test = train_test_split(X_0, y_0, test_size = 0.2, random_state = 40)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 40)\ndata_val_1, data_test_1 = pd.concat([X_val, y_val], axis = 1), pd.concat([X_test, y_test], axis = 1)\n\n# Splitting the fraudulent class\nX_val, X_test, y_val, y_test = train_test_split(X_1, y_1, test_size = 0.5, random_state = 40)\ndata_val_2, data_test_2 = pd.concat([X_val, y_val], axis = 1), pd.concat([X_test, y_test], axis = 1)\n\n# Merging data to construct the validation set and the test set\ndata_val, data_test = pd.concat([data_val_1, data_val_2], axis = 0), pd.concat([data_test_1, data_test_2], axis = 0)\nX_val, y_val = data_val.drop('Class', axis = 1), data_val['Class']\nX_test, y_test = data_test.drop('Class', axis = 1), data_test['Class']","metadata":{"id":"BNejq6NXWX3l","execution":{"iopub.status.busy":"2024-03-05T21:53:36.168715Z","iopub.execute_input":"2024-03-05T21:53:36.169099Z","iopub.status.idle":"2024-03-05T21:53:36.43056Z","shell.execute_reply.started":"2024-03-05T21:53:36.169069Z","shell.execute_reply":"2024-03-05T21:53:36.429442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of authentic and fraudulent transactions over training, validation and test set\nlabels = ['Train', 'Validation', 'Test']\nvalues_0 = [len(y_train[y_train == 0]), len(y_val[y_val == 0]), len(y_test[y_test == 0])]\nvalues_1 = [len(y_train[y_train == 1]), len(y_val[y_val == 1]), len(y_test[y_test == 1])]\nfig = make_subplots(rows = 1, cols = 2, specs = [[{'type': 'domain'}, {'type': 'domain'}]])\nfig.add_trace(go.Pie(values = values_0, labels = labels, hole = 0.5, textinfo = 'percent', title = \"Authentic\"),\n              row = 1, col = 1)\nfig.add_trace(go.Pie(values = values_1, labels = labels, hole = 0.5, textinfo = 'percent', title = \"Fraudulent\"),\n              row = 1, col = 2)\ntext_title = \"Distribution of authentic and fraudulent transactions over training, validation and test set\"\nfig.update_layout(height = 500, width = 800, showlegend = True, title = dict(text = text_title, x = 0.5, y = 0.95)) \nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:36.432266Z","iopub.execute_input":"2024-03-05T21:53:36.432601Z","iopub.status.idle":"2024-03-05T21:53:36.659762Z","shell.execute_reply.started":"2024-03-05T21:53:36.432571Z","shell.execute_reply":"2024-03-05T21:53:36.65838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the number of bins\nbins_train = math.floor(len(X_train)**(1/3))","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:36.662122Z","iopub.execute_input":"2024-03-05T21:53:36.662595Z","iopub.status.idle":"2024-03-05T21:53:36.668699Z","shell.execute_reply.started":"2024-03-05T21:53:36.662551Z","shell.execute_reply":"2024-03-05T21:53:36.66748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Time","metadata":{}},{"cell_type":"code","source":"# Decomposing time\nfor df in [X_train, X_val, X_test]:\n    df['Day'], temp = df['Time'] // (24*60*60), df['Time'] % (24*60*60)\n    df['Hour'], temp = temp // (60*60), temp % (60*60)\n    df['Minute'], df['Second'] = temp // 60, temp % 60\nX_train[['Time', 'Day', 'Hour', 'Minute', 'Second']].head()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:36.670644Z","iopub.execute_input":"2024-03-05T21:53:36.671061Z","iopub.status.idle":"2024-03-05T21:53:36.843024Z","shell.execute_reply.started":"2024-03-05T21:53:36.671024Z","shell.execute_reply":"2024-03-05T21:53:36.841799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharey = False)\nsns.histplot(data = X_train, x = 'Time', bins = bins_train, ax = ax[0])\nsns.histplot(data = X_train, x = 'Hour', bins = 24, ax = ax[1])\nax[1].set_ylabel(\" \")\nplt.suptitle(\"Histograms of Time and Hour\", size = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:36.844759Z","iopub.execute_input":"2024-03-05T21:53:36.845181Z","iopub.status.idle":"2024-03-05T21:53:37.807031Z","shell.execute_reply.started":"2024-03-05T21:53:36.845148Z","shell.execute_reply":"2024-03-05T21:53:37.805655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformation of 'Amount'\nfor df in [X_train, X_val, X_test]:\n    df['Amount_transformed'] = np.log10(df['Amount'] + 0.001)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:37.809442Z","iopub.execute_input":"2024-03-05T21:53:37.810958Z","iopub.status.idle":"2024-03-05T21:53:37.827578Z","shell.execute_reply.started":"2024-03-05T21:53:37.810912Z","shell.execute_reply":"2024-03-05T21:53:37.825786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharey = False)\nsns.histplot(data = X_train, x = 'Amount', bins = bins_train, ax = ax[0])\nsns.histplot(data = X_train, x = 'Amount_transformed', bins = bins_train, ax = ax[1])\nax[1].set_ylabel(\" \")\nplt.suptitle(\"Histograms of Amount and Amount_transformed\", size = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:37.834244Z","iopub.execute_input":"2024-03-05T21:53:37.834654Z","iopub.status.idle":"2024-03-05T21:53:38.99917Z","shell.execute_reply.started":"2024-03-05T21:53:37.83462Z","shell.execute_reply":"2024-03-05T21:53:38.997467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Discarding unnecessary columns\nfor df in [X_train, X_val, X_test]:\n    df.drop(['Time', 'Day', 'Minute', 'Second', 'Amount'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:39.001151Z","iopub.execute_input":"2024-03-05T21:53:39.001656Z","iopub.status.idle":"2024-03-05T21:53:39.159289Z","shell.execute_reply.started":"2024-03-05T21:53:39.001607Z","shell.execute_reply":"2024-03-05T21:53:39.158151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"code","source":"# Comparison of feature distributions for different target classes\ndata_val = pd.concat([X_val, y_val], axis = 1)\ndata_val_0, data_val_1 = data_val[data_val['Class'] == 0], data_val[data_val['Class'] == 1]\ncols, ncols = list(X_val.columns), 3\nnrows = math.ceil(len(cols) / ncols)\nfig, ax = plt.subplots(nrows, ncols, figsize = (4.5 * ncols, 4 * nrows))\nfor i in range(len(cols)):\n    sns.kdeplot(data_val_0[cols[i]], ax = ax[i // ncols, i % ncols])\n    sns.kdeplot(data_val_1[cols[i]], ax = ax[i // ncols, i % ncols])\n    if i % ncols != 0:\n        ax[i // ncols, i % ncols].set_ylabel(\" \")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:39.161039Z","iopub.execute_input":"2024-03-05T21:53:39.162519Z","iopub.status.idle":"2024-03-05T21:53:51.576934Z","shell.execute_reply.started":"2024-03-05T21:53:39.162468Z","shell.execute_reply":"2024-03-05T21:53:51.575467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature selection\ncols = ['V4', 'V11', 'V12', 'V14', 'V16', 'V17', 'V18', 'V19', 'Hour']\nX_train_fs, X_val_fs, X_test_fs = X_train[cols], X_val[cols], X_test[cols]\nX_train_fs.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:51.578564Z","iopub.execute_input":"2024-03-05T21:53:51.579Z","iopub.status.idle":"2024-03-05T21:53:51.617428Z","shell.execute_reply.started":"2024-03-05T21:53:51.578939Z","shell.execute_reply":"2024-03-05T21:53:51.615874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementing Anomaly Detection","metadata":{}},{"cell_type":"markdown","source":"The [**probability density function**](https://en.wikipedia.org/wiki/Probability_density_function) (pdf) of a univariate normal distribution with mean $\\mu$ and standard deviation $\\sigma$ is given by\n\n$$ f\\left(x; \\mu, \\sigma\\right) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\\,\\, \\exp\\left(-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2\\right), $$\n\nfor $x \\in \\mathbb{R}$, where $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$.","metadata":{}},{"cell_type":"code","source":"# Normal pdf\ndef normal_density(x, mu, sigma):\n    \"\"\"\n    Computes univariate normal probability density function (pdf) with mean mu, standard deviation sigma\n    Args:\n      x (scalar)    : input observation\n      mu (scalar)   : mean\n      sigma (scalar): standard deviation (> 0)\n    Returns:\n      f (scalar): value of the univariate normal pdf\n    \"\"\"\n    assert sigma > 0, \"Standard deviation must be positive\"\n    f = (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(- (1 / 2) * ((x - mu) / sigma)**2)\n    return f","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:51.618852Z","iopub.execute_input":"2024-03-05T21:53:51.619561Z","iopub.status.idle":"2024-03-05T21:53:51.629283Z","shell.execute_reply.started":"2024-03-05T21:53:51.619515Z","shell.execute_reply":"2024-03-05T21:53:51.627727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Product of normal pdfs\ndef normal_product(x_vec, mu_vec, sigma_vec):\n    \"\"\"\n    Computes product of univariate normal densities\n    Args:\n      x_vec (array_like, shape (n,))    : vector of input observations\n      mu_vec (array_like, shape (n,))   : vector of means\n      sigma_vec (array_like, shape (n,)): vector of standard deviations (> 0)\n    Returns:\n      f (scalar): product of univariate normal densities\n    \"\"\"\n    assert min(sigma_vec) > 0, \"Standard deviation must be positive\"\n    assert len(mu_vec) == len(x_vec), \"Length of mean vector does not match length of input vector\"\n    assert len(sigma_vec) == len(x_vec), \"Length of standard deviation vector does not match length of input vector\"\n    f = 1\n    for i in range(len(x_vec)):\n        f = f * normal_density(x_vec[i], mu_vec[i], sigma_vec[i])\n    return f","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:51.631067Z","iopub.execute_input":"2024-03-05T21:53:51.631418Z","iopub.status.idle":"2024-03-05T21:53:51.641537Z","shell.execute_reply.started":"2024-03-05T21:53:51.631386Z","shell.execute_reply":"2024-03-05T21:53:51.640376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model fitting\nmu_train, sigma_train = X_train_fs.mean().values, X_train_fs.std().values","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:51.643212Z","iopub.execute_input":"2024-03-05T21:53:51.643588Z","iopub.status.idle":"2024-03-05T21:53:51.701838Z","shell.execute_reply.started":"2024-03-05T21:53:51.643553Z","shell.execute_reply":"2024-03-05T21:53:51.700553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we predict anomaly based on a given threshold $\\epsilon$ for probability density in the following way:\n\n\\begin{align*}\n&y = 0\\,\\, \\text{(not anomaly)},\\,\\, \\text{if}\\,\\, g\\left(\\boldsymbol{x}; \\boldsymbol{\\mu}_{\\text{train}}, \\boldsymbol{\\sigma}_{\\text{train}}\\right) \\geq \\epsilon,\\\\\n&y = 1\\,\\, \\text{(anomaly)},\\,\\, \\text{otherwise}.\n\\end{align*}","metadata":{}},{"cell_type":"code","source":"# Function to predict anomaly based on probability density threshold\ndef model_normal(X, epsilon):\n    \"\"\"\n    Anomaly detection model\n    Args:\n      X (DataFrame, shape (m, n)): DataFrame of features\n      epsilon (scalar)           : threshold density value (> 0)\n    Returns:\n      y (array_like, shape (m,)): predicted class labels\n    \"\"\"\n    y = []\n    for i in X.index:\n        prob_density = normal_product(X.loc[i].tolist(), mu_train, sigma_train)\n        y.append((prob_density < epsilon).astype(int))\n    return y","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:51.703805Z","iopub.execute_input":"2024-03-05T21:53:51.704263Z","iopub.status.idle":"2024-03-05T21:53:51.712832Z","shell.execute_reply.started":"2024-03-05T21:53:51.704225Z","shell.execute_reply":"2024-03-05T21:53:51.711204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Threshold Tuning on Validation Set","metadata":{}},{"cell_type":"markdown","source":"First, we construct some functions to compute and display the confusion matrix and to compute the $F_2$-score, given the true labels and the predicted labels of the target.","metadata":{}},{"cell_type":"code","source":"# Function to compute confusion matrix\ndef conf_mat(y_test, y_pred):\n    \"\"\"\n    Computes confusion matrix\n    Args:\n      y_test (array_like): true binary (0 or 1) labels\n      y_pred (array_like): predicted binary (0 or 1) labels\n    Returns:\n      confusion_mat (array): A 2D array representing a 2x2 confusion matrix\n    \"\"\"\n    y_test, y_pred = list(y_test), list(y_pred)\n    count, labels, confusion_mat = len(y_test), [0, 1], np.zeros(shape = (2, 2), dtype = int)\n    for i in range(2):\n        for j in range(2):\n            confusion_mat[i][j] = len([k for k in range(count) if y_test[k] == labels[i] and y_pred[k] == labels[j]])\n    return confusion_mat","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:51.714364Z","iopub.execute_input":"2024-03-05T21:53:51.714727Z","iopub.status.idle":"2024-03-05T21:53:51.725985Z","shell.execute_reply.started":"2024-03-05T21:53:51.714693Z","shell.execute_reply":"2024-03-05T21:53:51.724435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to print confusion matrix\ndef conf_mat_heatmap(y_test, y_pred):\n    \"\"\"\n    Prints confusion matrix\n    Args:\n      y_test (array_like): true binary (0 or 1) labels\n      y_pred (array_like): predicted binary (0 or 1) labels\n    Returns:\n      Nothing, prints a heatmap representing a 2x2 confusion matrix\n    \"\"\"\n    confusion_mat = conf_mat(y_test, y_pred)\n    labels, confusion_mat_df = [0, 1], pd.DataFrame(confusion_mat, range(2), range(2))\n    plt.figure(figsize = (6, 4.75))\n    sns.heatmap(confusion_mat_df, annot = True, annot_kws = {\"size\": 16}, fmt = 'd')\n    plt.xticks([0.5, 1.5], labels, rotation = 'horizontal')\n    plt.yticks([0.5, 1.5], labels, rotation = 'horizontal')\n    plt.xlabel(\"Predicted label\", fontsize = 14)\n    plt.ylabel(\"True label\", fontsize = 14)\n    plt.title(\"Confusion Matrix\", fontsize = 14)\n    plt.grid(False)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:51.728021Z","iopub.execute_input":"2024-03-05T21:53:51.728621Z","iopub.status.idle":"2024-03-05T21:53:51.741569Z","shell.execute_reply.started":"2024-03-05T21:53:51.72853Z","shell.execute_reply":"2024-03-05T21:53:51.740265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to compute and return F2-score\ndef f2_score(y_test, y_pred):\n    \"\"\"\n    Computes F2-score, given true and predicted binary (0 or 1) labels\n    Args:\n      y_test (array_like): true binary (0 or 1) labels\n      y_pred (array_like): predicted binary (0 or 1) labels\n    Returns:\n      f2 (float): F2-score obtained from y_test and y_pred\n    \"\"\"\n    confusion_mat = conf_mat(y_test, y_pred)\n    tn, fp, fn, tp = confusion_mat[0, 0], confusion_mat[0, 1], confusion_mat[1, 0], confusion_mat[1, 1]\n    f2 = (5 * tp) / ((5 * tp) + (4 * fn) + fp)\n    return f2","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:51.743787Z","iopub.execute_input":"2024-03-05T21:53:51.744274Z","iopub.status.idle":"2024-03-05T21:53:51.755766Z","shell.execute_reply.started":"2024-03-05T21:53:51.744234Z","shell.execute_reply":"2024-03-05T21:53:51.754216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We set a sequence of threshold values `alpha`: $0.001, 0.002, \\cdots, 0.05$. These values are for the pdf of a single feature. The corresponding threshold for the joint probability density is `alpha` to the $n$-th power, where $n$ is the number of features used in the model.\n\nFor each threshold, we compute the $F_2$-score to evaluate the model performance on the validation set. The validation $F_2$-score is plotted against the threshold `alpha`.","metadata":{}},{"cell_type":"code","source":"# Tuning the threshold of density value\nalpha_list, f2_list, f2_max, alpha_opt, y_val_pred_opt = [], [], 0.0, 0.0, np.zeros(len(y_val))\nfor alpha, j in itertools.product(np.arange(0.001, 0.051, 0.001), range(1)):\n    y_val_pred = model_normal(X_val_fs, epsilon = alpha**X_val_fs.shape[1])\n    f2 = f2_score(y_val, y_val_pred)\n    alpha_list.append(alpha)\n    f2_list.append(f2)\n    if f2 > f2_max:\n        alpha_opt = alpha\n        y_val_pred_opt = y_val_pred\n        f2_max = f2","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:53:51.757723Z","iopub.execute_input":"2024-03-05T21:53:51.758847Z","iopub.status.idle":"2024-03-05T21:59:58.906327Z","shell.execute_reply.started":"2024-03-05T21:53:51.75879Z","shell.execute_reply":"2024-03-05T21:59:58.905223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting F2-score over alpha\nplt.figure(figsize = (9, 6))\nplt.plot(alpha_list, f2_list)\nplt.xlabel(\"alpha\", fontsize = 14)\nplt.ylabel(\"F2-score\", fontsize = 14)\nplt.title(\"F2-score vs alpha\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:59:58.907811Z","iopub.execute_input":"2024-03-05T21:59:58.909025Z","iopub.status.idle":"2024-03-05T21:59:59.189691Z","shell.execute_reply.started":"2024-03-05T21:59:58.908983Z","shell.execute_reply":"2024-03-05T21:59:59.188154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tuning summary\nprint(pd.Series({\n    \"Optimal alpha\": alpha_opt,\n    \"Optimal F2-score\": f2_score(y_val, y_val_pred_opt)\n}).to_string())","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:59:59.191377Z","iopub.execute_input":"2024-03-05T21:59:59.191746Z","iopub.status.idle":"2024-03-05T21:59:59.253405Z","shell.execute_reply.started":"2024-03-05T21:59:59.191715Z","shell.execute_reply":"2024-03-05T21:59:59.251925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix for predictions on the validation set\nconf_mat_heatmap(y_val, y_val_pred_opt)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:59:59.257471Z","iopub.execute_input":"2024-03-05T21:59:59.257906Z","iopub.status.idle":"2024-03-05T21:59:59.529924Z","shell.execute_reply.started":"2024-03-05T21:59:59.257873Z","shell.execute_reply":"2024-03-05T21:59:59.528433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and Evaluation on Test Set","metadata":{}},{"cell_type":"code","source":"# Function to compute and print evaluation metrics\ndef evaluation(y_test, y_pred):\n    confusion_mat = conf_mat(y_test, y_pred)\n    tn, fp, fn, tp = confusion_mat[0, 0], confusion_mat[0, 1], confusion_mat[1, 0], confusion_mat[1, 1]\n    print(pd.Series({\n        \"Accuracy\": (tp + tn) / (tn + fp + fn + tp),\n        \"Precision\": tp / (tp + fp),\n        \"Recall\": tp / (tp + fn),\n        \"F1-score\": (2 * tp) / ((2 * tp) + fn + fp),\n        \"F2-score\": (5 * tp) / ((5 * tp) + (4 * fn) + fp),\n        \"MCC\": ((tp * tn) - (fp * fn)) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n    }).to_string())","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:59:59.531495Z","iopub.execute_input":"2024-03-05T21:59:59.531852Z","iopub.status.idle":"2024-03-05T21:59:59.544549Z","shell.execute_reply.started":"2024-03-05T21:59:59.53182Z","shell.execute_reply":"2024-03-05T21:59:59.542964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction and evaluation on the test set\ny_test_normal = model_normal(X_test_fs, epsilon = alpha_opt**X_test_fs.shape[1])\nevaluation(y_test, y_test_normal)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T21:59:59.5466Z","iopub.execute_input":"2024-03-05T21:59:59.547867Z","iopub.status.idle":"2024-03-05T22:00:06.877763Z","shell.execute_reply.started":"2024-03-05T21:59:59.547819Z","shell.execute_reply":"2024-03-05T22:00:06.876284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix for predictions on the test set\nconf_mat_heatmap(y_test, y_test_normal)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T22:00:06.88014Z","iopub.execute_input":"2024-03-05T22:00:06.880565Z","iopub.status.idle":"2024-03-05T22:00:07.159822Z","shell.execute_reply.started":"2024-03-05T22:00:06.88053Z","shell.execute_reply":"2024-03-05T22:00:07.15846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"code","source":"# Runtime and memory usage\nstop = time.time()\nprint(pd.Series({\"Process runtime\": \"{:.2f} seconds\".format(float(stop - start)),\n                 \"Process memory usage\": \"{:.2f} MB\".format(float(process.memory_info()[0]/(1024*1024)))}).to_string())","metadata":{"execution":{"iopub.status.busy":"2024-03-05T22:00:07.16689Z","iopub.execute_input":"2024-03-05T22:00:07.168262Z","iopub.status.idle":"2024-03-05T22:00:07.180718Z","shell.execute_reply.started":"2024-03-05T22:00:07.168203Z","shell.execute_reply":"2024-03-05T22:00:07.178229Z"},"trusted":true},"execution_count":null,"outputs":[]}]}